{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning & DQNs (30 points + 5 bonus points)\n",
    "\n",
    "In this section, we will implement a few key parts of the Q-Learning algorithm for two cases - (1) A Q-network which is a single linear layer (referred to in RL literature as \"Q-learning with linear function approximation\") and (2) A deep (convolutional) Q-network, for some Atari game environments where the states are images.\n",
    "\n",
    "Optional Readings: \n",
    "- **Playing Atari with Deep Reinforcement Learning**, Mnih et. al., https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "- **The PyTorch DQN Tutorial** https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "\n",
    "Note: ** The bonus credit for this question applies to both sections CS 7643 and CS 4803**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from core.dqn_train import DQNTrain\n",
    "from utils.test_env import EnvTest\n",
    "from utils.schedule import LinearExploration, LinearSchedule\n",
    "from utils.preprocess import greyscale\n",
    "from utils.wrappers import PreproWrapper, MaxAndSkipEnv\n",
    "\n",
    "from linear_qnet import LinearQNet\n",
    "from cnn_qnet import ConvQNet\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda', 0)\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup Q-Learning with Linear Function Approximation\n",
    "\n",
    "Training Q-networks using (Deep) Q-learning involves a lot of moving parts. However, for this assignment, the scaffolding for the first 3 points listed below is provided in full and you must only complete point 4. You may skip to point 4 if you only care about the implementation required for this assignment.\n",
    "\n",
    "1. **Environments**: We will use the standardized OpenAI Gym framework for environment API calls (read through http://gym.openai.com/docs/ if you want to know more details about this interface). Specifically, we will use a custom Test environment defined in `utils/test_env.py` for initial sanity checks and then Gym-Atari environments later on.\n",
    "\n",
    "\n",
    "2. **Exploration**: In order to train any RL model, we require experience or \"data\" gathered from interacting with the environment by taking actions. What policy should we use to collect this experience? Given a Q-network, one may be tempted to define a greedy policy which always picks the highest valued action at every state. However, this strategy will in most cases not work since we may get stuck in a local minima and never explore new states in the environment which may lead to a better reward. Hence, for the purpose of gathering experience (or \"data\") from the environment, it is useful to follow a policy that deviates from the greedy policy slightly in order to explore new states. A common strategy used in RL is to follow an $\\epsilon$-greedy policy which with probability $0 < \\epsilon < 1$ picks a random action instead of the action provided by the greedy policy.\n",
    "\n",
    "\n",
    "3. **Replay Buffers**: Data gathered from a single trajectory of states and actions in the environment provides us with a batch of highly correlated (non IID) data, which leads to high variance in gradient updates and convergence. In order to ameliorate this, replay buffers are used to gather a set of transitions i.e. (state, action, reward, next state) tuples, by executing multiple trajectories in the environment. Now, for updating the Q-Network, we will first wait to fill up our replay buffer with a sufficiently large number of transitions over multiple different trajectories, and then randomly sample a batch of transitions to compute loss and update the models.\n",
    "\n",
    "\n",
    "4. **Q-Learning network, loss and update**: Finally, we come to the part of Q-learning that we will implement for this assignment -- the Q-network, loss function and update. In particular, we will implement a variant of Q-Learning called \"Double Q-Learning\", where we will maintain two Q networks -- the first Q network is used to pick actions and the second \"target\" Q network is used to compute Q-values for the picked actions. Here is some referance material on the same - [Blog 1](https://towardsdatascience.com/double-q-learning-the-easy-way-a924c4085ec3), [Blog 2](https://medium.com/@ameetsd97/deep-double-q-learning-why-you-should-use-it-bedf660d5295), but we will not need to get into the details of Double Q-learning for this assignment. Now, let's walk through the steps required to implement this below.\n",
    "\n",
    "    - **Linear Q-Network**: In `linear_qnet.py`, define the initialization and forward pass of a Q-network with a single linear layer which takes the state as input and outputs the Q-values for all actions.\n",
    "    - **Setting up Q-Learning**: In `core/dqn_train.py`, complete the functions `process_state`, `forward_loss` and `update_step` and `update_target_params`. The loss function for our Q-Networks is defined for a single transition tuple of (state, action, reward, next state) as follows. $Q(s_t, a_t)$ refers to the state-action values computed by our first Q-network at the current state and and for the current actions, $Q_{target}(s_{t+1}, a_{t+1})$ refers to the state-action values for the next state and all possible future actions computed by the target Q-Network\n",
    "$$\n",
    "\\begin{align*}\n",
    "    Q_{sample}(s_t) &= r_t \\textrm{ if done} \\hspace{10cm}\\\\\n",
    "    &= r_t + \\gamma \\max_{a_{t+1}} Q_{target}\\left(s_{t+1}, a_{t+1}\\right) \\textrm{ otherwise}\\\\\n",
    "    \\textrm{Loss} &= \\left( Q_{sample}(s_t) - Q(s_t, a_t) \\right) ^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Deliverable 1 (15 points)\n",
    "\n",
    "Run the following block of code to train a Linear Q-Network. You should get an average reward of ~4.0, full credit will be given if average reward at the final evaluation is above 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 1.90 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1001/10000 [==>...........................] - ETA: 3s - Loss: 0.1178 - Avg_R: 0.4450 - Max_R: 3.9000 - eps: 0.8020 - Grads: 0.5682 - Max_Q: 0.7528 - lr: 0.0042"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 4.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2001/10000 [=====>........................] - ETA: 3s - Loss: 0.3789 - Avg_R: 1.6900 - Max_R: 4.1000 - eps: 0.6040 - Grads: 0.6134 - Max_Q: 1.9634 - lr: 0.0034"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 3.90 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3001/10000 [========>.....................] - ETA: 3s - Loss: 0.3422 - Avg_R: 2.9600 - Max_R: 4.1000 - eps: 0.4060 - Grads: 0.6616 - Max_Q: 2.6036 - lr: 0.0026"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 3.90 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 4001/10000 [===========>..................] - ETA: 3s - Loss: 0.1746 - Avg_R: 2.8900 - Max_R: 4.1000 - eps: 0.2080 - Grads: 0.3340 - Max_Q: 2.6328 - lr: 0.0018"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average reward: 3.80 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5001/10000 [==============>...............] - ETA: 2s - Loss: 0.0355 - Avg_R: 4.0300 - Max_R: 4.1000 - eps: 0.0100 - Grads: 0.3054 - Max_Q: 2.5065 - lr: 0.0010"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 4.10 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 6001/10000 [=================>............] - ETA: 2s - Loss: 0.0032 - Avg_R: 4.1000 - Max_R: 4.1000 - eps: 0.0100 - Grads: 0.0435 - Max_Q: 2.4588 - lr: 0.0010"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average reward: 4.10 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7001/10000 [====================>.........] - ETA: 1s - Loss: 0.0081 - Avg_R: 4.0950 - Max_R: 4.1000 - eps: 0.0100 - Grads: 0.1895 - Max_Q: 2.7702 - lr: 0.0010"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average reward: 4.10 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8001/10000 [=======================>......] - ETA: 1s - Loss: 0.0028 - Avg_R: 4.1000 - Max_R: 4.1000 - eps: 0.0100 - Grads: 0.0747 - Max_Q: 2.6333 - lr: 0.0010"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 4.10 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 9001/10000 [==========================>...] - ETA: 0s - Loss: 0.0036 - Avg_R: 4.1000 - Max_R: 4.1000 - eps: 0.0100 - Grads: 0.1199 - Max_Q: 2.5523 - lr: 0.0010"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 4.10 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10001/10000 [==============================] - 5s - Loss: 0.0006 - Avg_R: 3.9900 - Max_R: 4.1000 - eps: 0.0100 - Grads: 0.0555 - Max_Q: 2.5246 - lr: 0.0010     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Training done.\n",
      "Evaluating...\n",
      "Average reward: 4.10 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from configs.p1_linear import config as config_lin\n",
    "\n",
    "env = EnvTest((5, 5, 1))\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "# np.random.seed(0)\n",
    "\n",
    "# exploration strategy\n",
    "exp_schedule = LinearExploration(env, config_lin.eps_begin,\n",
    "        config_lin.eps_end, config_lin.eps_nsteps)\n",
    "\n",
    "# learning rate schedule\n",
    "lr_schedule  = LinearSchedule(config_lin.lr_begin, config_lin.lr_end,\n",
    "        config_lin.lr_nsteps)\n",
    "\n",
    "# train model\n",
    "model = DQNTrain(LinearQNet, env, config_lin, device)\n",
    "model.run(exp_schedule, lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a final average reward of over 4.0 on the test environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Q-Learning with Deep Q-Networks\n",
    "\n",
    "In `cnn_qnet.py`, implement the initialization and forward pass of a convolutional Q-network with architecture as described in this DeepMind paper:\n",
    "    \n",
    "\"Playing Atari with Deep Reinforcement Learning\", Mnih et. al. (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)\n",
    "\n",
    "### Deliverable 2 (10 points)\n",
    "\n",
    "Run the following block of code to train our Deep Q-Network. You should get an average reward of ~4.0, full credit will be given if average reward at the final evaluation is above 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 1.40 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the memory 150/200..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 1.40 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 301/1000 [========>.....................] - ETA: 1s - Loss: 0.1094 - Avg_R: 0.0600 - Max_R: 2.0000 - eps: 0.4060 - Grads: 1.7718 - Max_Q: 0.1945 - lr: 0.0002"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: -0.10 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 401/1000 [===========>..................] - ETA: 2s - Loss: 0.0122 - Avg_R: 0.5550 - Max_R: 2.2000 - eps: 0.2080 - Grads: 0.6975 - Max_Q: 0.2228 - lr: 0.0001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 0.50 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 501/1000 [==============>...............] - ETA: 2s - Loss: 0.0465 - Avg_R: 0.2250 - Max_R: 1.4000 - eps: 0.0100 - Grads: 0.6939 - Max_Q: 0.2306 - lr: 0.0001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 0.50 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 601/1000 [=================>............] - ETA: 1s - Loss: 0.2469 - Avg_R: 2.9200 - Max_R: 4.1000 - eps: 0.0100 - Grads: 2.7119 - Max_Q: 0.3206 - lr: 0.0001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 4.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 701/1000 [====================>.........] - ETA: 1s - Loss: 0.0860 - Avg_R: 3.9900 - Max_R: 4.0000 - eps: 0.0100 - Grads: 1.6148 - Max_Q: 0.4457 - lr: 0.0001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average reward: 4.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 801/1000 [=======================>......] - ETA: 1s - Loss: 0.2162 - Avg_R: 3.6500 - Max_R: 4.0000 - eps: 0.0100 - Grads: 3.8238 - Max_Q: 0.5501 - lr: 0.0001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average reward: 3.90 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 901/1000 [==========================>...] - ETA: 0s - Loss: 0.0735 - Avg_R: 3.4550 - Max_R: 4.0000 - eps: 0.0100 - Grads: 1.8827 - Max_Q: 0.6370 - lr: 0.0001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average reward: 3.80 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1001/1000 [==============================] - 5s - Loss: 0.0160 - Avg_R: 3.8350 - Max_R: 4.0000 - eps: 0.0100 - Grads: 1.4425 - Max_Q: 0.7116 - lr: 0.0001     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Training done.\n",
      "Evaluating...\n",
      "Average reward: 4.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from configs.p2_cnn import config as config_cnn\n",
    "\n",
    "env = EnvTest((80, 80, 1))\n",
    "\n",
    "# exploration strategy\n",
    "exp_schedule = LinearExploration(env, config_cnn.eps_begin,\n",
    "        config_cnn.eps_end, config_cnn.eps_nsteps)\n",
    "\n",
    "# learning rate schedule\n",
    "lr_schedule  = LinearSchedule(config_cnn.lr_begin, config_cnn.lr_end,\n",
    "        config_cnn.lr_nsteps)\n",
    "\n",
    "# train model\n",
    "model = DQNTrain(ConvQNet, env, config_cnn, device)\n",
    "model.run(exp_schedule, lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a final average reward of over 4.0 on the test environment, similar to the previous case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Playing Atari Games from Pixels - using Linear Function Approximation\n",
    "\n",
    "Now that we have setup our Q-Learning algorithm and tested it on a simple test environment, we will shift to a harder environment - an Atari 2600 game from OpenAI Gym: Pong-v0 (https://gym.openai.com/envs/Pong-v0/), where we will use RGB images of the game screen as our observations for state.\n",
    "\n",
    "No additional implementation is required for this part, just run the block of code below (will take around 1 hour to train). We don't expect a simple linear Q-network to do well on such a hard environment - full credit will be given simply for running the training to completion irrespective of the final average reward obtained.\n",
    "\n",
    "You may edit `configs/p3_train_atari_linear.py` if you wish to play around with hyperparamters for improving performance of the linear Q-network on Pong-v0, or try another Atari environment by changing the `env_name` hyperparameter. The list of all Gym Atari environments are available here: https://gym.openai.com/envs/#atari\n",
    "\n",
    "### Deliverable 3 (5 points)\n",
    "\n",
    "Run the following block of code to train a linear Q-network on Atari Pong-v0. We don't expect the linear Q-Network to learn anything meaingful so full credit will be given for simply running this training to completion (without errors), irrespective of the final average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Q-Net Architecture:\n",
      " LinearQNet(\n",
      "  (linear_layer): Linear(in_features=25600, out_features=6, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-51e302efe344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLinearQNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_lina\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Linear Q-Net Architecture:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_schedule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_schedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/MSCS/DL/DL_Fall2019/HW4/hw4_starter/q_learning/core/q_train.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, exp_schedule, lr_schedule)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;31m# model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_schedule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_schedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;31m# record one game at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MSCS/DL/DL_Fall2019/HW4/hw4_starter/q_learning/core/q_train.py\u001b[0m in \u001b[0;36mdo_training\u001b[0;34m(self, exp_schedule, lr_schedule)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_record\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# time control of nb of steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mscores_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# list of scores computed at iteration time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mscores_eval\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mprog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProgbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnsteps_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MSCS/DL/DL_Fall2019/HW4/hw4_starter/q_learning/core/q_train.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, env, num_episodes)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0;31m# store last state in buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m                     \u001b[0mq_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_recent_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MSCS/DL/DL_Fall2019/HW4/hw4_starter/q_learning/utils/replay_buffer.py\u001b[0m in \u001b[0;36mstore_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \"\"\"\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from configs.p3_train_atari_linear import config as config_lina\n",
    "\n",
    "# make env\n",
    "env = gym.make(config_lina.env_name)\n",
    "env = MaxAndSkipEnv(env, skip=config_lina.skip_frame)\n",
    "env = PreproWrapper(env, prepro=greyscale, shape=(80, 80, 1),\n",
    "                    overwrite_render=config_lina.overwrite_render)\n",
    "\n",
    "# exploration strategy\n",
    "exp_schedule = LinearExploration(env, config_lina.eps_begin,\n",
    "        config_lina.eps_end, config_lina.eps_nsteps)\n",
    "\n",
    "# learning rate schedule\n",
    "lr_schedule  = LinearSchedule(config_lina.lr_begin, config_lina.lr_end,\n",
    "        config_lina.lr_nsteps)\n",
    "\n",
    "# train model\n",
    "model = DQNTrain(LinearQNet, env, config_lina, device)\n",
    "print(\"Linear Q-Net Architecture:\\n\", model.q_net)\n",
    "model.run(exp_schedule, lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: \\[BONUS\\] Playing Atari Games from Pixels - using Deep Q-Networks\n",
    "\n",
    "This part is extra credit and worth 5 bonus points. We will now train our deep Q-Network from Part 2 on Pong-v0. \n",
    "\n",
    "Again, no additional implementation is required but you may wish to tweak your CNN architecture in `cnn_qnet.py` and hyperparameters in `configs/p4_train_atari_cnn.py` (however, evaluation will be considered at no farther than the default 5 million steps, so you are not allowed to train for longer). Please note that this training may take a very long time (we tested this on a single GPU and it took around 6 hours).\n",
    "\n",
    "The bonus points for this question will be allotted based on the best evaluation average reward (EAR) before 5 million time stpes:\n",
    "\n",
    "1. EAR >= 0.0 : 4/4 points\n",
    "2. EAR >= -5.0 : 3/4 points\n",
    "3. EAR >= -10.0 : 3/4 points\n",
    "4. EAR >= -15.0 : 1/4 points\n",
    "\n",
    "### Deliverable 4: (5 bonus points)\n",
    "\n",
    "Run the following block of code to train your DQN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from configs.p4_train_atari_cnn import config as config_cnna\n",
    "\n",
    "\n",
    "# make env\n",
    "env = gym.make(config_cnna.env_name)\n",
    "env = MaxAndSkipEnv(env, skip=config_cnna.skip_frame)\n",
    "env = PreproWrapper(env, prepro=greyscale, shape=(80, 80, 1),\n",
    "                    overwrite_render=config_cnna.overwrite_render)\n",
    "\n",
    "# exploration strategy\n",
    "exp_schedule = LinearExploration(env, config_cnna.eps_begin,\n",
    "        config_cnna.eps_end, config_cnna.eps_nsteps)\n",
    "\n",
    "# learning rate schedule\n",
    "lr_schedule  = LinearSchedule(config_cnna.lr_begin, config_cnna.lr_end,\n",
    "        config_cnna.lr_nsteps)\n",
    "\n",
    "# train model\n",
    "model = DQNTrain(ConvQNet, env, config_cnna, device)\n",
    "print(\"CNN Q-Net Architecture:\\n\", model.q_net)\n",
    "model.run(exp_schedule, lr_schedule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
